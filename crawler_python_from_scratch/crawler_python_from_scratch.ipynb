{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始学Python网络爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This spam is absolutely horrible.\n",
      "3.14\n",
      "https://bd/com\n",
      "['http://abc/p1/', 'http://abc/p2/', 'http://abc/p3/', 'http://abc/p4/', 'http://abc/p5/', 'http://abc/p6/', 'http://abc/p7/', 'http://abc/p8/', 'http://abc/p9/']\n"
     ]
    }
   ],
   "source": [
    "## format（）， 格式化函数\n",
    "# 在 str.format() 调用时使用关键字参数，可以通过参数名来引用值\n",
    "print('This {food} is {adjective}.'.format(food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "print(\"{:.2f}\".format(3.1415926))\n",
    "\n",
    "\"{} {}\".format(\"hello\", \"world\")    # 不设置指定位置，按默认顺序\n",
    "\n",
    "path = 'https://{}/{}'.format(\"bd\", \"com\")\n",
    "print(path)\n",
    "\n",
    "url = ['http://abc/p{}/'.format(number) for number in range(1,10)]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫原理\n",
    "网络连接需要一次Requests请求和服务器端的Response回应。爬虫原理：  \n",
    "- 模拟电脑对服务器发起Requests请求\n",
    "- 接收服务器端的Response的内容并解析、提取所需信息  \n",
    "\n",
    "常用的两种爬虫的流程：多页面和跨页面爬虫流程。\n",
    "![](./note/flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫三大库\n",
    "- Requests  \n",
    "Requests库的错误和异常。\n",
    " - ConnectionError：网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    " - HTTPError：HTTP错误异常，比如网页不存在，返回404\n",
    " - URLRequired：URL缺失异常\n",
    " - TooManyRedirects：超过最大重定向次数，产生重定向异常\n",
    " - ConnectTimeout：连接远程服务器超时异常\n",
    " - Timeout：请求URL超时，产生超时异常\n",
    "- BeautifulSoup  \n",
    "解析器： html.parser、lxml等，用法: BeautifulSoup(url.text, \"html.parser\"),BeautifulSoup(url.text, \"lxml\")\n",
    "- Lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "## requests\n",
    "import requests\n",
    "\n",
    "# 加入请求头，伪装成浏览器\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_links(url):\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    print(wb_data.status_code)\n",
    "    try:\n",
    "        print(wb_data)\n",
    "        # print(wb_data.text)\n",
    "    except ConnectonError:\n",
    "        print('Requests Error')\n",
    "\n",
    "# test    \n",
    "url = \"http://www.baidu.com\"\n",
    "get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>488</i> \t 488\n",
      "<i>498</i> \t 498\n",
      "<i>498</i> \t 498\n",
      "<i>340</i> \t 340\n",
      "<i>498</i> \t 498\n",
      "<i>458</i> \t 458\n",
      "<i>528</i> \t 528\n",
      "<i>430</i> \t 430\n",
      "<i>388</i> \t 388\n",
      "<i>278</i> \t 278\n",
      "<i>498</i> \t 498\n",
      "<i>388</i> \t 388\n",
      "<i>369</i> \t 369\n",
      "<i>300</i> \t 300\n",
      "<i>598</i> \t 598\n",
      "<i>608</i> \t 608\n",
      "<i>408</i> \t 408\n",
      "<i>398</i> \t 398\n",
      "<i>278</i> \t 278\n",
      "<i>298</i> \t 298\n",
      "<i>428</i> \t 428\n",
      "<i>278</i> \t 278\n",
      "<i>418</i> \t 418\n",
      "<i>609</i> \t 609\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup， select方法\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = \"http://bj.xiaozhu.com/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# 选择房价的元素的路径（右键 > Copy Selector），得到房价值\n",
    "prices = soup.select('#page_list > ul > li > div.result_btm_con.lodgeunitname > div > span > i')\n",
    "for price in prices:\n",
    "    print(price, \"\\t\", price.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"resule_img_a\" href=\"http://bj.xiaozhu.com/fangzi/29968007503.html\" target=\"_blank\">\n",
      "<img alt=\"实木北欧品质两居，近十里河地铁，肿瘤医院\" class=\"lodgeunitpic\" data-growing-title=\"29968007503\" lazy_src=\"https://image.xiaozhustatic3.com/12/14,0,92,4210,3000,2000,a7296326.jpg\" src=\"../images/lazy_loadimage.png\" title=\"实木北欧品质两居，近十里河地铁，肿瘤医院\"/>\n",
      "</a> \n",
      "\n",
      " http://bj.xiaozhu.com/fangzi/29968007503.html\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup\n",
    "# 需要的url如下， \n",
    "# <a target=\"_blank\" href=\"http://bj.xiaozhu.com/fangzi/29968007503.html\" class=\"resule_img_a\">\n",
    "# Element的Selector： page_list > ul > li:nth-child(1) > a\n",
    "url = \"http://bj.xiaozhu.com/search-duanzufang-p2-0/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "link = soup.select('#page_list > ul > li > a')\n",
    "## 用相同的select方法，得到了该级元素的内容，即<a...</a>\n",
    "#+ 然后用get(element_name)方法，获得\"href\"的属性值\n",
    "print(link[0], 2*\"\\n\", link[0].get(\"href\"))\n",
    "\n",
    "## 同时，我们可以进一步用相同的方法提取： title、img等信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取酷狗Top500的数据\n",
    "- url： https://www.kugou.com/yy/rank/home/1-8888.html?from=rank\n",
    "- 代码： [kugou.py](./book_src/kugou.py),用Python3运行 （修复了原代码书中的一个bug）\n",
    "- 思路： (1)观察翻页的各页url主入口如何获取； (2)分别在各页爬取，方法是： requests+BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式： Python re模块\n",
    "- search()\n",
    "- sub()\n",
    "- findall()  \n",
    "\n",
    "可以用正则表达式直接解析返回的html文件，得到有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='one'> \n",
      " one \n",
      "\n",
      "one two three \n",
      "\n",
      " ['one', 'two', 'three']\n",
      "\n",
      " ['123', '456', '789']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "## re.search()\n",
    "a = \"one1two2three3\"\n",
    "info = re.search('\\D+', a)\n",
    "print(info, \"\\n\", info.group(), \"\\n\")\n",
    "\n",
    "## re.sub()\n",
    "new_info = re.sub('\\d+', ' ', a)\n",
    "print(new_info)\n",
    "\n",
    "## re.findall()\n",
    "infos = re.findall('\\D+', a)\n",
    "print(\"\\n\", infos)\n",
    "\n",
    "a= \"<a>123</a><a>456</a><a>789</a>\"\n",
    "## 边界匹配，括号里的内容作为返回结果\n",
    "infos = re.findall('<a>(.*?)</a>', a)\n",
    "print(\"\\n\", infos)\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取《斗破苍穹》全文小说\n",
    "- url: http://www.doupoxs.com/doupocangqiong/\n",
    "- 代码： [doupo_xiaoshuo.py](./book_src/doupo_xiaoshuo.py),用Python3运行\n",
    "- `content.decode('utf-8')`\n",
    "- `re.S`， re修饰符， 匹配包含换行在内的所有字符\n",
    "- `time.sleep(1)`， 防止请求频率过快导致爬虫失败  \n",
    "注： **最好加入一个try/except判断，如果请求因过快被拒绝，则重新连接， 保证数据的完整性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取糗事百科的段子\n",
    "- url: https://www.qiushibaike.com/text/\n",
    "- 代码： [qiushibaike.py](./book_src/qiushibaike.py),用Python3运行\n",
    "- P.S. 对段子中的`\"</br>\"`字符串，需要替换删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
