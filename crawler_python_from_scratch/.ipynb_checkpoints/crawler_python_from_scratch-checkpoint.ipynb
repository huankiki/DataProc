{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始学Python网络爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This spam is absolutely horrible.\n",
      "3.14\n",
      "https://bd/com\n",
      "['http://abc/p1/', 'http://abc/p2/', 'http://abc/p3/', 'http://abc/p4/', 'http://abc/p5/', 'http://abc/p6/', 'http://abc/p7/', 'http://abc/p8/', 'http://abc/p9/']\n"
     ]
    }
   ],
   "source": [
    "## format（）， 格式化函数\n",
    "# 在 str.format() 调用时使用关键字参数，可以通过参数名来引用值\n",
    "print('This {food} is {adjective}.'.format(food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "print(\"{:.2f}\".format(3.1415926))\n",
    "\n",
    "\"{} {}\".format(\"hello\", \"world\")    # 不设置指定位置，按默认顺序\n",
    "\n",
    "path = 'https://{}/{}'.format(\"bd\", \"com\")\n",
    "print(path)\n",
    "\n",
    "url = ['http://abc/p{}/'.format(number) for number in range(1,10)]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫原理\n",
    "网络连接需要一次Requests请求和服务器端的Response回应。爬虫原理：  \n",
    "- 模拟电脑对服务器发起Requests请求\n",
    "- 接收服务器端的Response的内容并解析、提取所需信息  \n",
    "\n",
    "常用的两种爬虫的流程：多页面和跨页面爬虫流程。\n",
    "![](./note/flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫三大库\n",
    "- **Requests**  \n",
    "Requests库的错误和异常：\n",
    " - ConnectionError：网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    " - HTTPError：HTTP错误异常，比如网页不存在，返回404\n",
    " - URLRequired：URL缺失异常\n",
    " - TooManyRedirects：超过最大重定向次数，产生重定向异常\n",
    " - ConnectTimeout：连接远程服务器超时异常\n",
    " - Timeout：请求URL超时，产生超时异常\n",
    "- **BeautifulSoup**  \n",
    "解析器： html.parser、lxml等，用法: BeautifulSoup(url.text, \"html.parser\"),BeautifulSoup(url.text, \"lxml\")  \n",
    "**在浏览器中可以得到BeautifulSoup的select方法的路径： 右键 > Copy Selector**。\n",
    "- **Lxml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "## requests\n",
    "import requests\n",
    "\n",
    "# 加入请求头，伪装成浏览器\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_links(url):\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    print(wb_data.status_code)\n",
    "    try:\n",
    "        print(wb_data)\n",
    "        # print(wb_data.text)\n",
    "    except ConnectonError:\n",
    "        print('Requests Error')\n",
    "\n",
    "# test    \n",
    "url = \"http://www.baidu.com\"\n",
    "get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>488</i> \t 488\n",
      "<i>498</i> \t 498\n",
      "<i>518</i> \t 518\n",
      "<i>340</i> \t 340\n",
      "<i>498</i> \t 498\n",
      "<i>388</i> \t 388\n",
      "<i>388</i> \t 388\n",
      "<i>458</i> \t 458\n",
      "<i>278</i> \t 278\n",
      "<i>430</i> \t 430\n",
      "<i>369</i> \t 369\n",
      "<i>498</i> \t 498\n",
      "<i>528</i> \t 528\n",
      "<i>300</i> \t 300\n",
      "<i>408</i> \t 408\n",
      "<i>598</i> \t 598\n",
      "<i>339</i> \t 339\n",
      "<i>278</i> \t 278\n",
      "<i>298</i> \t 298\n",
      "<i>428</i> \t 428\n",
      "<i>278</i> \t 278\n",
      "<i>418</i> \t 418\n",
      "<i>609</i> \t 609\n",
      "<i>498</i> \t 498\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup， select方法\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = \"http://bj.xiaozhu.com/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# 选择房价的元素的路径（右键 > Copy Selector），得到房价值\n",
    "prices = soup.select('#page_list > ul > li > div.result_btm_con.lodgeunitname > div > span > i')\n",
    "for price in prices:\n",
    "    print(price, \"\\t\", price.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"resule_img_a\" href=\"http://bj.xiaozhu.com/fangzi/10573777860.html\" target=\"_blank\">\n",
      "<img alt=\"近国贸百子湾地铁-经典黑白loft-可拍摄\" class=\"lodgeunitpic\" data-growing-title=\"10573777860\" lazy_src=\"https://image.xiaozhustatic3.com/12/10,0,43,4243,1800,1201,c8e51b89.jpg\" src=\"../images/lazy_loadimage.png\" title=\"近国贸百子湾地铁-经典黑白loft-可拍摄\"/>\n",
      "</a> \n",
      "\n",
      " http://bj.xiaozhu.com/fangzi/10573777860.html\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup\n",
    "# 需要的url如下， \n",
    "# <a target=\"_blank\" href=\"http://bj.xiaozhu.com/fangzi/29968007503.html\" class=\"resule_img_a\">\n",
    "# Element的Selector： page_list > ul > li:nth-child(1) > a\n",
    "url = \"http://bj.xiaozhu.com/search-duanzufang-p2-0/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "link = soup.select('#page_list > ul > li > a')\n",
    "## 用相同的select方法，得到了该级元素的内容，即<a...</a>\n",
    "#+ 然后用get(element_name)方法，获得\"href\"属性值\n",
    "print(link[0], 2*\"\\n\", link[0].get(\"href\"))\n",
    "\n",
    "## 同时，我们可以进一步用相同的方法提取： title、img等信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取酷狗Top500的数据\n",
    "**方法是： requests+BeautifulSoup**\n",
    "- url： https://www.kugou.com/yy/rank/home/1-8888.html?from=rank\n",
    "- 代码： [kugou.py](./book_src/kugou.py),用Python3运行 （修复了原代码书中的一个bug）\n",
    "- 思路： (1)观察翻页的各页url主入口如何获取； (2)分别在各页爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式： Python re模块\n",
    "- search()\n",
    "- sub()\n",
    "- findall()  \n",
    "\n",
    "可以用正则表达式直接解析返回的html文件，得到有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='one'> \n",
      " one \n",
      "\n",
      "one two three \n",
      "\n",
      " ['one', 'two', 'three']\n",
      "\n",
      " ['123', '456', '789']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "## re.search()\n",
    "a = \"one1two2three3\"\n",
    "info = re.search('\\D+', a)\n",
    "print(info, \"\\n\", info.group(), \"\\n\")\n",
    "\n",
    "## re.sub()\n",
    "new_info = re.sub('\\d+', ' ', a)\n",
    "print(new_info)\n",
    "\n",
    "## re.findall()\n",
    "infos = re.findall('\\D+', a)\n",
    "print(\"\\n\", infos)\n",
    "\n",
    "a= \"<a>123</a><a>456</a><a>789</a>\"\n",
    "## 边界匹配，括号里的内容作为返回结果\n",
    "infos = re.findall('<a>(.*?)</a>', a)\n",
    "print(\"\\n\", infos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取《斗破苍穹》全文小说\n",
    "**方法是： requests+re**\n",
    "- url: http://www.doupoxs.com/doupocangqiong/\n",
    "- 代码： [doupo_xiaoshuo.py](./book_src/doupo_xiaoshuo.py),用Python3运行\n",
    "- `content.decode('utf-8')`\n",
    "- `re.S`， re修饰符， 匹配包含换行在内的所有字符\n",
    "- `time.sleep(1)`， 防止请求频率过快导致爬虫失败  \n",
    "注： **最好加入一个try/except判断，如果请求因过快被拒绝，则重新连接， 保证数据的完整性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取糗事百科的段子\n",
    "**方法是： requests+re**\n",
    "- url: https://www.qiushibaike.com/text/\n",
    "- 代码： [qiushibaike.py](./book_src/qiushibaike.py),用Python3运行\n",
    "- P.S. 对段子中的`\"</br>\"`字符串，需要替换删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lxml库 + Xpath语法\n",
    "lxml库是用来解析XML和HTML文件的一个Python库。  \n",
    "Xpath是一门在XML文档中查找信息的语言，同时支持HTML文档。  \n",
    "可参考： [Xpath-菜鸟教程](https://www.runoob.com/xpath/xpath-tutorial.html)  \n",
    "**在爬虫实战中，Xpath路径可以通过浏览器得到： 右键 > Copy Xpath。**  \n",
    "\n",
    "|表达式|描述/结果|\n",
    "|-|-|\n",
    "|/|从根节点选取|\n",
    "|//|从匹配的当前节点选取|\n",
    "|..|选取当前节点的父节点|\n",
    "|@|选取属性|\n",
    "|-|-|\n",
    "|/cnode|选取根元素cnode|\n",
    "|cnode/node|选取属于cnode的子元素的所有node元素|\n",
    "|//cnode|选取所有cnode子元素，不管它们在文档中的位置|\n",
    "|cnode//node|选取属于cnode的所有node子元素，不管它们位于cnode之下的什么位置|\n",
    "|//@attribute|选取名为attribute的所有属性|\n",
    "|//li[@attr]|选取所有拥有名为attr属性的li元素|\n",
    "|//li[@attr=\"red\"]|选取所有attr属性值为red的li元素|\n",
    "|/text()|获取标签中的文本|\n",
    "\n",
    "#### 比较三种方法：re/BeautifulSoup/Lxml\n",
    "- 代码： [compare.py](./book_src/compare.py)\n",
    "- 速度： Lxml ≈ 正则 > BeautifulSoup\n",
    "- 难度： Lxml ≈ BeautifulSoup < 正则  \n",
    "\n",
    "所以当爬取的数据量大，且需要快速实现时，**选择Lxml是最佳选择，因为速度快，实现简单。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好i八月\n",
      "秀的拽，你不配\n",
      "狸土豆大人\n",
      "李中东\n",
      "琉之璃\n",
      "不喜欢洗锅\n",
      "鹰嫂\n",
      "（我叫红包）\n",
      "吃个榴莲压压惊\n",
      "没有你生而何欢？\n",
      "偷惢\n",
      "隔壁ㄨ老王\n",
      "小呆妹！\n",
      "-小门神~\n",
      "youMyWay\n",
      "猪猪小二\n",
      "老巫婆～～\n",
      "内涵葮子TV、\n",
      "夕冬温存\n",
      "她要它爱他\n",
      "偷惢\n",
      "鱼歌浅唱\n",
      "-小门神~\n",
      "又一盏素酒\n",
      "查查猪吃\n"
     ]
    }
   ],
   "source": [
    "## Xpath， 获取糗事百科/文字中的一个页面下的所有user name\n",
    "import requests\n",
    "# lxml库\n",
    "from lxml import etree\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = 'http://www.qiushibaike.com/text/'\n",
    "res = requests.get(url,headers=headers)\n",
    "## etree.HTML\n",
    "selector = etree.HTML(res.text)\n",
    "usernames_path = selector.xpath('//div[@class=\"article block untagged mb15 typs_long\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_hot\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_old\"]')\n",
    "## 以上复杂的形式，可以用 starts-with 简化、替代\n",
    "usernames_path = selector.xpath('//div[starts-with(@class, \"article block untagged mb15\")]')\n",
    "\n",
    "for username_path in usernames_path:\n",
    "    if username_path.xpath('div[1]/a[2]/h2'):\n",
    "        ## 因为返回的是list，所以取唯一的一个元素即可， [0]\n",
    "        username = username_path.xpath('div[1]/a[2]/h2/text()')[0].strip()\n",
    "        print(username)\n",
    "    else:  # 匿名用户没有以上节点\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用 string(.) 获取标签套标签的文本内容\n",
    "[文言文-古诗文网](https://www.gushiwen.org/shiwen/default_4A444444444444A1.aspx)中的多段长文和只有一段的文言文的节点设置不同。  \n",
    "此时，为了把div节点下的所有文本都获取，可以用 string(.)解决该问题。    \n",
    "![](./note/stringall.png)  \n",
    "![](./note/stringall2.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******桃花源记******\n",
      "晋太元中，武陵人捕鱼为业。缘溪行，忘路之远近。忽逢桃花林，夹岸数百步，中无杂树，芳草鲜美，落英缤纷，渔人甚异之。复前行，欲穷其林。\n",
      "　　林尽水源，便得一山，山有小口，仿佛若有光。便舍船，从口入。初极狭，才通人。复行数十步，豁然开朗。土地平旷，屋舍俨然，有良田美池桑竹之属。阡陌交通，鸡犬相闻。其中往来种作，男女衣着，悉如外人。黄发垂髫，并怡然自乐。\n",
      "　　见渔人，乃大惊，问所从来。具答之。便要还家，设酒杀鸡作食。村中闻有此人，咸来问讯。自云先世避秦时乱，率妻子邑人来此绝境，不复出焉，遂与外人间隔。问今是何世，乃不知有汉，无论魏晋。此人一一为具言所闻，皆叹惋。余人各复延至其家，皆出酒食。停数日，辞去。此中人语云：“不足为外人道也。”(间隔 一作：隔绝)\n",
      "　　既出，得其船，便扶向路，处处志之。及郡下，诣太守，说如此。太守即遣人随其往，寻向所志，遂迷，不复得路。\n",
      "　　南阳刘子骥，高尚士也，闻之，欣然规往。未果，寻病终，后遂无问津者。\n",
      "\n",
      "******陋室铭******\n",
      "山不在高，有仙则名。水不在深，有龙则灵。斯是陋室，惟吾德馨。苔痕上阶绿，草色入帘青。谈笑有鸿儒，往来无白丁。可以调素琴，阅金经。无丝竹之乱耳，无案牍之劳形。南阳诸葛庐，西蜀子云亭。孔子云：何陋之有？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 函数， 代码复用\n",
    "def stringall(url, name):\n",
    "    res = requests.get(url, headers=headers)\n",
    "    selector = etree.HTML(res.text)\n",
    "    guwen_path = selector.xpath('//div[@class=\"contson\"]')[0]\n",
    "    ## string(.)\n",
    "    guwen = guwen_path.xpath('string(.)').strip()\n",
    "    info = \"******\" + name + \"******\"\n",
    "    print(info)\n",
    "    print(guwen)\n",
    "    print()\n",
    "    \n",
    "## 桃花源记\n",
    "url = \"https://so.gushiwen.org/shiwenv_73add8822103.aspx\"\n",
    "stringall(url, \"桃花源记\")\n",
    "\n",
    "## 陋室铭\n",
    "url = \"https://so.gushiwen.org/shiwenv_6c1ea9b7dd44.aspx\"\n",
    "stringall(url, \"陋室铭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取豆瓣图书Top250\n",
    "方法是： Requests + Lxml\n",
    "- 将结果存储为结构化的csv文件格式\n",
    "- 代码： [doubanbook.py](./book_src/doubanbook.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用API爬取数据\n",
    "- 返回的一般是JSON格式的数据， 需要用JSON解析库解析JSON数据\n",
    "- 可以用Requests库发起请求（GET或者POST）。但一般来说，开放的API有一些限制： 需要验证、或者有调用次数的限制、或者需要apikey才有调用权限。\n",
    "- **API爬虫很常见，比如开源的聊天机器人、天气、地图等等**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiaoming\n"
     ]
    }
   ],
   "source": [
    "## json\n",
    "import json\n",
    "jsonstring = '{\"name\":\"xiaoming\", \"gender\":\"man\"}'\n",
    "json_data = json.loads(jsonstring)\n",
    "print(json_data[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': '1', 'name': '张三'}\n",
      "{\"age\": \"1\", \"name\": \"张三\"}\n"
     ]
    }
   ],
   "source": [
    "# Python2会有中文编码问题， 打印json字符串，输出的中文就成了unicode码\n",
    "import json\n",
    "d = {'name': '张三', 'age': '1'}\n",
    "# Python3: {'name': '张三', 'age': '1'}\n",
    "# Python2: {'age': '1', 'name': '\\xe5\\xbc\\xa0\\xe4\\xb8\\x89'}\n",
    "print(d)\n",
    "# Python2: jd = json.dumps(d, ensure_ascii=False, encoding='utf-8'))\n",
    "jd = json.dumps(d, ensure_ascii=False)\n",
    "print(jd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据库存储\n",
    "- MongoDB数据库  \n",
    "MongoDB是一种非关系型数据库(NoSQL)。NoSQL可以解决大规模数据集合多重数据种类的问题。  \n",
    "NoSQL分为4大类： 键值存储数据库Redis、列存储数据库Hbase、文档型数据库MongoDB、图形数据库Graph。   \n",
    "数据库和集合，类似于Excel文件和其中的表格，一个数据库可以有多个集合。  \n",
    "    - 安装MongoDB数据库： sudo apt install mongodb-clients, sudo apt install mongodb-server\n",
    "    - 配置数据库： mongod -dbpath path\n",
    "    - 启动： mongo， 使用MongoDB期间， 启动服务的窗口不能关闭\n",
    "    - Python安装pymong库： pip3 install pymongo\n",
    "    - 导出数据： mongoexport -d mydb -c test --csv -f name,gender,age -o test.csv    \n",
    "\n",
    "- MySQL\n",
    "最流行的开源的关系型数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f65a01c8788>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost', 27017)  #连接数据库\n",
    "mydb = client['mydb']  #新建mydb数据库\n",
    "test = mydb[\"test\"]  #新建test数据集合\n",
    "# 删除一个集合中的所有文档\n",
    "result = test.delete_many({})\n",
    "test.insert_one({\"name\":\"Ming\", \"gender\":\"male\", \"age\":\"30\"})\n",
    "## 在MongoDB设置的路径下， 打开命令行窗口，运行以下命令导出数据\n",
    "# -d： 数据库， -c： 数据集合， -f： 导出的字段\n",
    "# mongoexport -d mydb -c test --csv -f name,gender,age -o test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取豆瓣音乐Top250、豆瓣电影Top250\n",
    "方法是： Requests + Lxml + re + **MongoDB**\n",
    "- 代码： [doubanmusic.py](./book_src/doubanmusic.py)\n",
    "- 修复了一个bug， 网页代码有变化\n",
    "- 导出结果： mongoexport -d mydb -c musictop --csv -f name,author,style,time,publisher,score -o douban_music.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多进程爬虫\n",
    "可以使用Python的multiprocessing库的进程池方法进行多进程爬虫。\n",
    "\n",
    "### 实践Task： 爬取58同城二手市场的商品信息\n",
    "方法是： Requests + Lxml + **MongoDB + Muitiprocessing/Pool**  \n",
    "- 代码： [58project/main.py](./book_src/58project/main.py)\n",
    "- 可以参考的一个思想，也是在实践中常常遇到的问题： 如果由于某些原因，爬虫程序中断，如何**不手动更改却可以实现断点续爬**？  \n",
    "代码中的实现方法是一个参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#导入multiprocessing库的Pool模块\n",
    "\n",
    "#创建进程池，processes参数为设置进程的个数\n",
    "pool = Pool(processes=4)\n",
    "\n",
    "# 用map()函数进行进程运行\n",
    "# func参数为需运行的函数，在爬虫实战中，为爬虫函数\n",
    "# iterable为迭代参数，在爬虫实战中，可为多个URL列表进行迭代。\n",
    "pool.map(func,urls)\n",
    "'''\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 爬虫原理\n",
    "结合实例，了解搭建爬虫的逻辑和思路，分析和拆解问题。\n",
    "#### 爬虫三大库\n",
    "- Requests， GET和POST请求\n",
    "- Lxml， Xpath方法\n",
    "- BeautifulSoup， select方法  \n",
    "\n",
    "#### Python re模块、json模块\n",
    "另外还有： time、 csv、 try/except、 pymongo、multiprocessing、\n",
    "\n",
    "#### 除了三大库，还有与爬虫相关的方方面面、细节 & 技巧\n",
    "- 数据库MongoDB\n",
    "- 多进程/多线程\n",
    "- 异常判断\n",
    "- 断点自动续爬\n",
    "- \n",
    "\n",
    "#### 爬虫实例汇总\n",
    "- 爬取糗事百科的段子\n",
    "- 爬取《斗破苍穹》全文小说\n",
    "- 爬取酷狗Top500的数据\n",
    "- 爬取豆瓣图书Top250、豆瓣音乐Top250、豆瓣电影Top250\n",
    "- 使用API爬取数据：百度地图、高德地图\n",
    "- 爬取58同城二手市场的商品信息\n",
    "- 常见方法：\n",
    "    - Requests + Lxml/re + MongoDB + Muitiprocessing/Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 与爬虫or文本处理有关的Python模块/库\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pymongo\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
