{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始学Python网络爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This spam is absolutely horrible.\n",
      "3.14\n",
      "https://bd/com\n",
      "['http://abc/p1/', 'http://abc/p2/', 'http://abc/p3/', 'http://abc/p4/', 'http://abc/p5/', 'http://abc/p6/', 'http://abc/p7/', 'http://abc/p8/', 'http://abc/p9/']\n"
     ]
    }
   ],
   "source": [
    "## format（）， 格式化函数\n",
    "# 在 str.format() 调用时使用关键字参数，可以通过参数名来引用值\n",
    "print('This {food} is {adjective}.'.format(food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "print(\"{:.2f}\".format(3.1415926))\n",
    "\n",
    "\"{} {}\".format(\"hello\", \"world\")    # 不设置指定位置，按默认顺序\n",
    "\n",
    "path = 'https://{}/{}'.format(\"bd\", \"com\")\n",
    "print(path)\n",
    "\n",
    "url = ['http://abc/p{}/'.format(number) for number in range(1,10)]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫原理\n",
    "网络连接需要一次Requests请求和服务器端的Response回应。爬虫原理：  \n",
    "- 模拟电脑对服务器发起Requests请求\n",
    "- 接收服务器端的Response的内容并解析、提取所需信息  \n",
    "\n",
    "常用的两种爬虫的流程：多页面和跨页面爬虫流程。\n",
    "![](./note/flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫三大库\n",
    "- **Requests**  \n",
    "Requests库的错误和异常：\n",
    " - ConnectionError：网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    " - HTTPError：HTTP错误异常，比如网页不存在，返回404\n",
    " - URLRequired：URL缺失异常\n",
    " - TooManyRedirects：超过最大重定向次数，产生重定向异常\n",
    " - ConnectTimeout：连接远程服务器超时异常\n",
    " - Timeout：请求URL超时，产生超时异常\n",
    "- **BeautifulSoup**  \n",
    "解析器： html.parser、lxml等，用法: BeautifulSoup(url.text, \"html.parser\"),BeautifulSoup(url.text, \"lxml\")  \n",
    "**在浏览器中可以得到BeautifulSoup的select方法的路径： 右键 > Copy Selector**。\n",
    "- **Lxml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "## requests\n",
    "import requests\n",
    "\n",
    "# 加入请求头，伪装成浏览器\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_links(url):\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    print(wb_data.status_code)\n",
    "    try:\n",
    "        print(wb_data)\n",
    "        # print(wb_data.text)\n",
    "    except ConnectonError:\n",
    "        print('Requests Error')\n",
    "\n",
    "# test    \n",
    "url = \"http://www.baidu.com\"\n",
    "get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>488</i> \t 488\n",
      "<i>498</i> \t 498\n",
      "<i>340</i> \t 340\n",
      "<i>498</i> \t 498\n",
      "<i>498</i> \t 498\n",
      "<i>458</i> \t 458\n",
      "<i>430</i> \t 430\n",
      "<i>300</i> \t 300\n",
      "<i>388</i> \t 388\n",
      "<i>598</i> \t 598\n",
      "<i>278</i> \t 278\n",
      "<i>388</i> \t 388\n",
      "<i>369</i> \t 369\n",
      "<i>608</i> \t 608\n",
      "<i>408</i> \t 408\n",
      "<i>398</i> \t 398\n",
      "<i>498</i> \t 498\n",
      "<i>278</i> \t 278\n",
      "<i>298</i> \t 298\n",
      "<i>428</i> \t 428\n",
      "<i>278</i> \t 278\n",
      "<i>418</i> \t 418\n",
      "<i>609</i> \t 609\n",
      "<i>528</i> \t 528\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup， select方法\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = \"http://bj.xiaozhu.com/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# 选择房价的元素的路径（右键 > Copy Selector），得到房价值\n",
    "prices = soup.select('#page_list > ul > li > div.result_btm_con.lodgeunitname > div > span > i')\n",
    "for price in prices:\n",
    "    print(price, \"\\t\", price.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"resule_img_a\" href=\"http://bj.xiaozhu.com/fangzi/2597512263.html\" target=\"_blank\">\n",
      "<img alt=\"百子湾小白 LOFT 近地铁 公寓 做饭双井\" class=\"lodgeunitpic\" data-growing-title=\"2597512263\" lazy_src=\"https://image.xiaozhustatic3.com/12/51,0,16,114635,3080,2000,a2cc1f01.jpg\" src=\"../images/lazy_loadimage.png\" title=\"百子湾小白 LOFT 近地铁 公寓 做饭双井\"/>\n",
      "</a> \n",
      "\n",
      " http://bj.xiaozhu.com/fangzi/2597512263.html\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup\n",
    "# 需要的url如下， \n",
    "# <a target=\"_blank\" href=\"http://bj.xiaozhu.com/fangzi/29968007503.html\" class=\"resule_img_a\">\n",
    "# Element的Selector： page_list > ul > li:nth-child(1) > a\n",
    "url = \"http://bj.xiaozhu.com/search-duanzufang-p2-0/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "link = soup.select('#page_list > ul > li > a')\n",
    "## 用相同的select方法，得到了该级元素的内容，即<a...</a>\n",
    "#+ 然后用get(element_name)方法，获得\"href\"属性值\n",
    "print(link[0], 2*\"\\n\", link[0].get(\"href\"))\n",
    "\n",
    "## 同时，我们可以进一步用相同的方法提取： title、img等信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取酷狗Top500的数据\n",
    "**方法是： requests+BeautifulSoup**\n",
    "- url： https://www.kugou.com/yy/rank/home/1-8888.html?from=rank\n",
    "- 代码： [kugou.py](./book_src/kugou.py),用Python3运行 （修复了原代码书中的一个bug）\n",
    "- 思路： (1)观察翻页的各页url主入口如何获取； (2)分别在各页爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式： Python re模块\n",
    "- search()\n",
    "- sub()\n",
    "- findall()  \n",
    "\n",
    "可以用正则表达式直接解析返回的html文件，得到有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='one'> \n",
      " one \n",
      "\n",
      "one two three \n",
      "\n",
      " ['one', 'two', 'three']\n",
      "\n",
      " ['123', '456', '789']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "## re.search()\n",
    "a = \"one1two2three3\"\n",
    "info = re.search('\\D+', a)\n",
    "print(info, \"\\n\", info.group(), \"\\n\")\n",
    "\n",
    "## re.sub()\n",
    "new_info = re.sub('\\d+', ' ', a)\n",
    "print(new_info)\n",
    "\n",
    "## re.findall()\n",
    "infos = re.findall('\\D+', a)\n",
    "print(\"\\n\", infos)\n",
    "\n",
    "a= \"<a>123</a><a>456</a><a>789</a>\"\n",
    "## 边界匹配，括号里的内容作为返回结果\n",
    "infos = re.findall('<a>(.*?)</a>', a)\n",
    "print(\"\\n\", infos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取《斗破苍穹》全文小说\n",
    "**方法是： requests+re**\n",
    "- url: http://www.doupoxs.com/doupocangqiong/\n",
    "- 代码： [doupo_xiaoshuo.py](./book_src/doupo_xiaoshuo.py),用Python3运行\n",
    "- `content.decode('utf-8')`\n",
    "- `re.S`， re修饰符， 匹配包含换行在内的所有字符\n",
    "- `time.sleep(1)`， 防止请求频率过快导致爬虫失败  \n",
    "注： **最好加入一个try/except判断，如果请求因过快被拒绝，则重新连接， 保证数据的完整性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取糗事百科的段子\n",
    "**方法是： requests+re**\n",
    "- url: https://www.qiushibaike.com/text/\n",
    "- 代码： [qiushibaike.py](./book_src/qiushibaike.py),用Python3运行\n",
    "- P.S. 对段子中的`\"</br>\"`字符串，需要替换删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lxml库 + Xpath语法\n",
    "lxml库是用来解析XML和HTML文件的一个Python库。  \n",
    "Xpath是一门在XML文档中查找信息的语言，同时支持HTML文档。  \n",
    "可参考： [Xpath-菜鸟教程](https://www.runoob.com/xpath/xpath-tutorial.html)  \n",
    "**在爬虫实战中，Xpath路径可以通过浏览器得到： 右键 > Copy Xpath。**  \n",
    "\n",
    "|表达式|描述/结果|\n",
    "|-|-|\n",
    "|/|从根节点选取|\n",
    "|//|从匹配的当前节点选取|\n",
    "|..|选取当前节点的父节点|\n",
    "|@|选取属性|\n",
    "|-|-|\n",
    "|/cnode|选取根元素cnode|\n",
    "|cnode/node|选取属于cnode的子元素的所有node元素|\n",
    "|//cnode|选取所有cnode子元素，不管它们在文档中的位置|\n",
    "|cnode//node|选取属于cnode的所有node子元素，不管它们位于cnode之下的什么位置|\n",
    "|//@attribute|选取名为attribute的所有属性|\n",
    "|//li[@attr]|选取所有拥有名为attr属性的li元素|\n",
    "|//li[@attr=\"red\"]|选取所有attr属性值为red的li元素|\n",
    "|/text()|获取标签中的文本|\n",
    "\n",
    "#### 比较三种方法：re/BeautifulSoup/Lxml\n",
    "- 代码： [compare.py](./book_src/compare.py)\n",
    "- 速度： Lxml ≈ 正则 > BeautifulSoup\n",
    "- 难度： Lxml ≈ BeautifulSoup < 正则  \n",
    "\n",
    "所以当爬取的数据量大，且需要快速实现时，**选择Lxml是最佳选择，因为速度快，实现简单。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大爱浇水\n",
      "小呆妹！\n",
      "骑着二哈啃黄瓜\n",
      "无书斋主\n",
      "苗博文\n",
      "不喜欢洗锅\n",
      "吃了两碗又盛\n",
      "是谁辜负了好时光\n",
      "o为什么o\n",
      "*好大一棵树*\n",
      "吃了两碗又盛\n",
      "老巫婆～～\n",
      "狼族少年-\n",
      "小呆妹！\n",
      "漫步风雨人生路\n",
      "鹰嫂\n",
      "你好i八月\n",
      "偷惢\n",
      "-小门神~\n",
      "大部分都是自己\n",
      "浪中鱼\n",
      "时月～\n",
      "喂鱼抽喵\n",
      "花落彼岸孤独望殇\n",
      "削铅笔喽\n"
     ]
    }
   ],
   "source": [
    "## Xpath， 获取糗事百科/文字中的一个页面下的所有user name\n",
    "import requests\n",
    "# lxml库\n",
    "from lxml import etree\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = 'http://www.qiushibaike.com/text/'\n",
    "res = requests.get(url,headers=headers)\n",
    "## etree.HTML\n",
    "selector = etree.HTML(res.text)\n",
    "usernames_path = selector.xpath('//div[@class=\"article block untagged mb15 typs_long\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_hot\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_old\"]')\n",
    "## 以上复杂的形式，可以用 starts-with 简化、替代\n",
    "usernames_path = selector.xpath('//div[starts-with(@class, \"article block untagged mb15\")]')\n",
    "\n",
    "for username_path in usernames_path:\n",
    "    if username_path.xpath('div[1]/a[2]/h2'):\n",
    "        ## 因为返回的是list，所以取唯一的一个元素即可， [0]\n",
    "        username = username_path.xpath('div[1]/a[2]/h2/text()')[0].strip()\n",
    "        print(username)\n",
    "    else:  # 匿名用户没有以上节点\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用 string(.) 获取标签套标签的文本内容\n",
    "[文言文-古诗文网](https://www.gushiwen.org/shiwen/default_4A444444444444A1.aspx)中的多段长文和只有一段的文言文的节点设置不同。  \n",
    "此时，为了把div节点下的所有文本都获取，可以用 string(.)解决该问题。    \n",
    "![](./note/stringall.png)  \n",
    "![](./note/stringall2.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******桃花源记******\n",
      "晋太元中，武陵人捕鱼为业。缘溪行，忘路之远近。忽逢桃花林，夹岸数百步，中无杂树，芳草鲜美，落英缤纷，渔人甚异之。复前行，欲穷其林。\n",
      "　　林尽水源，便得一山，山有小口，仿佛若有光。便舍船，从口入。初极狭，才通人。复行数十步，豁然开朗。土地平旷，屋舍俨然，有良田美池桑竹之属。阡陌交通，鸡犬相闻。其中往来种作，男女衣着，悉如外人。黄发垂髫，并怡然自乐。\n",
      "　　见渔人，乃大惊，问所从来。具答之。便要还家，设酒杀鸡作食。村中闻有此人，咸来问讯。自云先世避秦时乱，率妻子邑人来此绝境，不复出焉，遂与外人间隔。问今是何世，乃不知有汉，无论魏晋。此人一一为具言所闻，皆叹惋。余人各复延至其家，皆出酒食。停数日，辞去。此中人语云：“不足为外人道也。”(间隔 一作：隔绝)\n",
      "　　既出，得其船，便扶向路，处处志之。及郡下，诣太守，说如此。太守即遣人随其往，寻向所志，遂迷，不复得路。\n",
      "　　南阳刘子骥，高尚士也，闻之，欣然规往。未果，寻病终，后遂无问津者。\n",
      "\n",
      "******陋室铭******\n",
      "山不在高，有仙则名。水不在深，有龙则灵。斯是陋室，惟吾德馨。苔痕上阶绿，草色入帘青。谈笑有鸿儒，往来无白丁。可以调素琴，阅金经。无丝竹之乱耳，无案牍之劳形。南阳诸葛庐，西蜀子云亭。孔子云：何陋之有？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 函数， 代码复用\n",
    "def stringall(url, name):\n",
    "    res = requests.get(url, headers=headers)\n",
    "    selector = etree.HTML(res.text)\n",
    "    guwen_path = selector.xpath('//div[@class=\"contson\"]')[0]\n",
    "    ## string(.)\n",
    "    guwen = guwen_path.xpath('string(.)').strip()\n",
    "    info = \"******\" + name + \"******\"\n",
    "    print(info)\n",
    "    print(guwen)\n",
    "    print()\n",
    "    \n",
    "## 桃花源记\n",
    "url = \"https://so.gushiwen.org/shiwenv_73add8822103.aspx\"\n",
    "stringall(url, \"桃花源记\")\n",
    "\n",
    "## 陋室铭\n",
    "url = \"https://so.gushiwen.org/shiwenv_6c1ea9b7dd44.aspx\"\n",
    "stringall(url, \"陋室铭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取豆瓣图书Top250\n",
    "方法是： Requests + Lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
