{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始学Python网络爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This spam is absolutely horrible.\n",
      "3.14\n",
      "https://bd/com\n",
      "['http://abc/p1/', 'http://abc/p2/', 'http://abc/p3/', 'http://abc/p4/', 'http://abc/p5/', 'http://abc/p6/', 'http://abc/p7/', 'http://abc/p8/', 'http://abc/p9/']\n"
     ]
    }
   ],
   "source": [
    "## format（）， 格式化函数\n",
    "# 在 str.format() 调用时使用关键字参数，可以通过参数名来引用值\n",
    "print('This {food} is {adjective}.'.format(food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "print(\"{:.2f}\".format(3.1415926))\n",
    "\n",
    "\"{} {}\".format(\"hello\", \"world\")    # 不设置指定位置，按默认顺序\n",
    "\n",
    "path = 'https://{}/{}'.format(\"bd\", \"com\")\n",
    "print(path)\n",
    "\n",
    "url = ['http://abc/p{}/'.format(number) for number in range(1,10)]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫原理\n",
    "网络连接需要一次Requests请求和服务器端的Response回应。爬虫原理：  \n",
    "- 模拟电脑对服务器发起Requests请求\n",
    "- 接收服务器端的Response的内容并解析、提取所需信息  \n",
    "\n",
    "常用的两种爬虫的流程：多页面和跨页面爬虫流程。\n",
    "![](./note/flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫三大库\n",
    "- **Requests**  \n",
    "Requests库的错误和异常：\n",
    " - ConnectionError：网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    " - HTTPError：HTTP错误异常，比如网页不存在，返回404\n",
    " - URLRequired：URL缺失异常\n",
    " - TooManyRedirects：超过最大重定向次数，产生重定向异常\n",
    " - ConnectTimeout：连接远程服务器超时异常\n",
    " - Timeout：请求URL超时，产生超时异常\n",
    "- **BeautifulSoup**  \n",
    "解析器： html.parser、lxml等，用法: BeautifulSoup(url.text, \"html.parser\"),BeautifulSoup(url.text, \"lxml\")  \n",
    "**在浏览器中可以得到BeautifulSoup的select方法的路径： 右键 > Copy Selector**。\n",
    "- **Lxml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "## requests\n",
    "import requests\n",
    "\n",
    "# 加入请求头，伪装成浏览器\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_links(url):\n",
    "    wb_data = requests.get(url,headers=headers)\n",
    "    print(wb_data.status_code)\n",
    "    try:\n",
    "        print(wb_data)\n",
    "        # print(wb_data.text)\n",
    "    except ConnectonError:\n",
    "        print('Requests Error')\n",
    "\n",
    "# test    \n",
    "url = \"http://www.baidu.com\"\n",
    "get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<i>488</i> \t 488\n",
      "<i>388</i> \t 388\n",
      "<i>388</i> \t 388\n",
      "<i>458</i> \t 458\n",
      "<i>278</i> \t 278\n",
      "<i>498</i> \t 498\n",
      "<i>419</i> \t 419\n",
      "<i>369</i> \t 369\n",
      "<i>498</i> \t 498\n",
      "<i>528</i> \t 528\n",
      "<i>300</i> \t 300\n",
      "<i>298</i> \t 298\n",
      "<i>609</i> \t 609\n",
      "<i>599</i> \t 599\n",
      "<i>408</i> \t 408\n",
      "<i>598</i> \t 598\n",
      "<i>339</i> \t 339\n",
      "<i>428</i> \t 428\n",
      "<i>278</i> \t 278\n",
      "<i>418</i> \t 418\n",
      "<i>278</i> \t 278\n",
      "<i>400</i> \t 400\n",
      "<i>498</i> \t 498\n",
      "<i>448</i> \t 448\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup， select方法\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = \"http://bj.xiaozhu.com/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# 选择房价的元素的路径（右键 > Copy Selector），得到房价值\n",
    "prices = soup.select('#page_list > ul > li > div.result_btm_con.lodgeunitname > div > span > i')\n",
    "for price in prices:\n",
    "    print(price, \"\\t\", price.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"resule_img_a\" href=\"http://bj.xiaozhu.com/fangzi/2597552363.html\" target=\"_blank\">\n",
      "<img alt=\"西域 复式 2人地铁近 做饭 可发票 出差～\" class=\"lodgeunitpic\" data-growing-title=\"2597552363\" lazy_src=\"https://image.xiaozhustatic3.com/12/51,0,77,114178,3000,2000,9006ac99.jpg\" src=\"../images/lazy_loadimage.png\" title=\"西域 复式 2人地铁近 做饭 可发票 出差～\"/>\n",
      "</a> \n",
      "\n",
      " http://bj.xiaozhu.com/fangzi/2597552363.html\n"
     ]
    }
   ],
   "source": [
    "## BeautifulSoup\n",
    "# 需要的url如下， \n",
    "# <a target=\"_blank\" href=\"http://bj.xiaozhu.com/fangzi/29968007503.html\" class=\"resule_img_a\">\n",
    "# Element的Selector： page_list > ul > li:nth-child(1) > a\n",
    "url = \"http://bj.xiaozhu.com/search-duanzufang-p2-0/\"\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "link = soup.select('#page_list > ul > li > a')\n",
    "## 用相同的select方法，得到了该级元素的内容，即<a...</a>\n",
    "#+ 然后用get(element_name)方法，获得\"href\"属性值\n",
    "print(link[0], 2*\"\\n\", link[0].get(\"href\"))\n",
    "\n",
    "## 同时，我们可以进一步用相同的方法提取： title、img等信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取酷狗Top500的数据\n",
    "**方法是： requests+BeautifulSoup**\n",
    "- url： https://www.kugou.com/yy/rank/home/1-8888.html?from=rank\n",
    "- 代码： [kugou.py](./book_src/kugou.py),用Python3运行 （修复了原代码书中的一个bug）\n",
    "- 思路： (1)观察翻页的各页url主入口如何获取； (2)分别在各页爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式： Python re模块\n",
    "- search()\n",
    "- sub()\n",
    "- findall()  \n",
    "\n",
    "可以用正则表达式直接解析返回的html文件，得到有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='one'> \n",
      " one \n",
      "\n",
      "one two three \n",
      "\n",
      " ['one', 'two', 'three']\n",
      "\n",
      " ['123', '456', '789']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "## re.search()\n",
    "a = \"one1two2three3\"\n",
    "info = re.search('\\D+', a)\n",
    "print(info, \"\\n\", info.group(), \"\\n\")\n",
    "\n",
    "## re.sub()\n",
    "new_info = re.sub('\\d+', ' ', a)\n",
    "print(new_info)\n",
    "\n",
    "## re.findall()\n",
    "infos = re.findall('\\D+', a)\n",
    "print(\"\\n\", infos)\n",
    "\n",
    "a= \"<a>123</a><a>456</a><a>789</a>\"\n",
    "## 边界匹配，括号里的内容作为返回结果\n",
    "infos = re.findall('<a>(.*?)</a>', a)\n",
    "print(\"\\n\", infos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取《斗破苍穹》全文小说\n",
    "**方法是： requests+re**\n",
    "- url: http://www.doupoxs.com/doupocangqiong/\n",
    "- 代码： [doupo_xiaoshuo.py](./book_src/doupo_xiaoshuo.py),用Python3运行\n",
    "- `content.decode('utf-8')`\n",
    "- `re.S`， re修饰符， 匹配包含换行在内的所有字符\n",
    "- `time.sleep(1)`， 防止请求频率过快导致爬虫失败  \n",
    "注： **最好加入一个try/except判断，如果请求因过快被拒绝，则重新连接， 保证数据的完整性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取糗事百科的段子\n",
    "**方法是： requests+re**\n",
    "- url: https://www.qiushibaike.com/text/\n",
    "- 代码： [qiushibaike.py](./book_src/qiushibaike.py),用Python3运行\n",
    "- P.S. 对段子中的`\"</br>\"`字符串，需要替换删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lxml库 + Xpath语法\n",
    "lxml库是用来解析XML和HTML文件的一个Python库。  \n",
    "Xpath是一门在XML文档中查找信息的语言，同时支持HTML文档。  \n",
    "可参考： [Xpath-菜鸟教程](https://www.runoob.com/xpath/xpath-tutorial.html)  \n",
    "**在爬虫实战中，Xpath路径可以通过浏览器得到： 右键 > Copy Xpath。**  \n",
    "\n",
    "|表达式|描述/结果|\n",
    "|-|-|\n",
    "|/|从根节点选取|\n",
    "|//|从匹配的当前节点选取|\n",
    "|..|选取当前节点的父节点|\n",
    "|@|选取属性|\n",
    "|-|-|\n",
    "|/cnode|选取根元素cnode|\n",
    "|cnode/node|选取属于cnode的子元素的所有node元素|\n",
    "|//cnode|选取所有cnode子元素，不管它们在文档中的位置|\n",
    "|cnode//node|选取属于cnode的所有node子元素，不管它们位于cnode之下的什么位置|\n",
    "|//@attribute|选取名为attribute的所有属性|\n",
    "|//li[@attr]|选取所有拥有名为attr属性的li元素|\n",
    "|//li[@attr=\"red\"]|选取所有attr属性值为red的li元素|\n",
    "|/text()|获取标签中的文本|\n",
    "\n",
    "#### 比较三种方法：re/BeautifulSoup/Lxml\n",
    "- 代码： [compare.py](./book_src/compare.py)\n",
    "- 速度： Lxml ≈ 正则 > BeautifulSoup\n",
    "- 难度： Lxml ≈ BeautifulSoup < 正则  \n",
    "\n",
    "所以当爬取的数据量大，且需要快速实现时，**选择Lxml是最佳选择，因为速度快，实现简单。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小小獠\n",
      "阳光倾城暖谁心\n",
      "孤犬落寞吠\n",
      "未识已终\n",
      "lonely仓鼠\n",
      "乌漆嘛黑的夜\n",
      "义行天下7\n",
      "鹰嫂\n",
      "不喜欢洗锅\n",
      "千年瀑布\n",
      "小呆妹！\n",
      "偷惢\n",
      "夕冬温存\n",
      "手倦拋书\n",
      "冬天的铁门很甜\n",
      "偷惢\n",
      "-小门神~\n",
      "吃个榴莲压压惊\n",
      "黄山小妖\n",
      "12312a\n",
      "饮最烈的酒、艹最爱…\n",
      "又一盏素酒\n",
      "秀的拽，你不配\n",
      "惊鸿一剑\n",
      "歌薇洛洗护潮州妹\n"
     ]
    }
   ],
   "source": [
    "## Xpath， 获取糗事百科/文字中的一个页面下的所有user name\n",
    "import requests\n",
    "# lxml库\n",
    "from lxml import etree\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'\n",
    "}\n",
    "url = 'http://www.qiushibaike.com/text/'\n",
    "res = requests.get(url,headers=headers)\n",
    "## etree.HTML\n",
    "selector = etree.HTML(res.text)\n",
    "usernames_path = selector.xpath('//div[@class=\"article block untagged mb15 typs_long\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_hot\" or  \\\n",
    "                                @class=\"article block untagged mb15 typs_old\"]')\n",
    "## 以上复杂的形式，可以用 starts-with 简化、替代\n",
    "usernames_path = selector.xpath('//div[starts-with(@class, \"article block untagged mb15\")]')\n",
    "\n",
    "for username_path in usernames_path:\n",
    "    if username_path.xpath('div[1]/a[2]/h2'):\n",
    "        ## 因为返回的是list，所以取唯一的一个元素即可， [0]\n",
    "        username = username_path.xpath('div[1]/a[2]/h2/text()')[0].strip()\n",
    "        print(username)\n",
    "    else:  # 匿名用户没有以上节点\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用 string(.) 获取标签套标签的文本内容\n",
    "[文言文-古诗文网](https://www.gushiwen.org/shiwen/default_4A444444444444A1.aspx)中的多段长文和只有一段的文言文的节点设置不同。  \n",
    "此时，为了把div节点下的所有文本都获取，可以用 string(.)解决该问题。    \n",
    "![](./note/stringall.png)  \n",
    "![](./note/stringall2.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******桃花源记******\n",
      "　　晋太元中，武陵人捕鱼为业。缘溪行，忘路之远近。忽逢桃花林，夹岸数百步，中无杂树，芳草鲜美，落英缤纷，渔人甚异之。复前行，欲穷其林。\n",
      "　　林尽水源，便得一山，山有小口，仿佛若有光。便舍船，从口入。初极狭，才通人。复行数十步，豁然开朗。土地平旷，屋舍俨然，有良田美池桑竹之属。阡陌交通，鸡犬相闻。其中往来种作，男女衣着，悉如外人。黄发垂髫，并怡然自乐。\n",
      "　　见渔人，乃大惊，问所从来。具答之。便要还家，设酒杀鸡作食。村中闻有此人，咸来问讯。自云先世避秦时乱，率妻子邑人来此绝境，不复出焉，遂与外人间隔。问今是何世，乃不知有汉，无论魏晋。此人一一为具言所闻，皆叹惋。余人各复延至其家，皆出酒食。停数日，辞去。此中人语云：“不足为外人道也。”(间隔 一作：隔绝)\n",
      "　　既出，得其船，便扶向路，处处志之。及郡下，诣太守，说如此。太守即遣人随其往，寻向所志，遂迷，不复得路。\n",
      "　　南阳刘子骥，高尚士也，闻之，欣然规往。未果，寻病终，后遂无问津者。\n",
      "\n",
      "******陋室铭******\n",
      "山不在高，有仙则名。水不在深，有龙则灵。斯是陋室，惟吾德馨。苔痕上阶绿，草色入帘青。谈笑有鸿儒，往来无白丁。可以调素琴，阅金经。无丝竹之乱耳，无案牍之劳形。南阳诸葛庐，西蜀子云亭。孔子云：何陋之有？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 函数， 代码复用\n",
    "def stringall(url, name):\n",
    "    res = requests.get(url, headers=headers)\n",
    "    selector = etree.HTML(res.text)\n",
    "    guwen_path = selector.xpath('//div[@class=\"contson\"]')[0]\n",
    "    ## string(.)\n",
    "    guwen = guwen_path.xpath('string(.)').strip(\"\\n\")\n",
    "    info = \"******\" + name + \"******\"\n",
    "    print(info)\n",
    "    print(guwen)\n",
    "    print()\n",
    "    \n",
    "## 桃花源记\n",
    "url = \"https://so.gushiwen.org/shiwenv_73add8822103.aspx\"\n",
    "stringall(url, \"桃花源记\")\n",
    "\n",
    "## 陋室铭\n",
    "url = \"https://so.gushiwen.org/shiwenv_6c1ea9b7dd44.aspx\"\n",
    "stringall(url, \"陋室铭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取豆瓣图书Top250\n",
    "方法是： Requests + Lxml\n",
    "- 将结果存储为结构化的csv文件格式\n",
    "- 代码： [doubanbook.py](./book_src/doubanbook.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用API爬取数据\n",
    "- 返回的一般是JSON格式的数据， 需要用JSON解析库解析JSON数据\n",
    "- 可以用Requests库发起请求（GET或者POST）。但一般来说，开放的API有一些限制： 需要验证、或者有调用次数的限制、或者需要apikey才有调用权限。\n",
    "- **API爬虫很常见，比如开源的聊天机器人、天气、地图等等**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiaoming\n"
     ]
    }
   ],
   "source": [
    "## json\n",
    "import json\n",
    "jsonstring = '{\"name\":\"xiaoming\", \"gender\":\"man\"}'\n",
    "json_data = json.loads(jsonstring)\n",
    "print(json_data[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '张三', 'age': '1'}\n",
      "{\"name\": \"张三\", \"age\": \"1\"}\n"
     ]
    }
   ],
   "source": [
    "# Python2会有中文编码问题， 打印json字符串，输出的中文就成了unicode码\n",
    "import json\n",
    "d = {'name': '张三', 'age': '1'}\n",
    "# Python3: {'name': '张三', 'age': '1'}\n",
    "# Python2: {'age': '1', 'name': '\\xe5\\xbc\\xa0\\xe4\\xb8\\x89'}\n",
    "print(d)\n",
    "# Python2: jd = json.dumps(d, ensure_ascii=False, encoding='utf-8'))\n",
    "jd = json.dumps(d, ensure_ascii=False)\n",
    "print(jd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据库存储\n",
    "- MongoDB数据库  \n",
    "MongoDB是一种非关系型数据库(NoSQL)。NoSQL可以解决大规模数据集合多重数据种类的问题。  \n",
    "NoSQL分为4大类： 键值存储数据库Redis、列存储数据库Hbase、文档型数据库MongoDB、图形数据库Graph。   \n",
    "数据库和集合，类似于Excel文件和其中的表格，一个数据库可以有多个集合。  \n",
    "    - 安装MongoDB数据库： sudo apt install mongodb-clients, sudo apt install mongodb-server\n",
    "    - 配置数据库： mongod -dbpath path\n",
    "    - 启动： mongo， 使用MongoDB期间， 启动服务的窗口不能关闭\n",
    "    - Python安装pymong库： pip3 install pymongo\n",
    "    - 导出数据： mongoexport -d mydb -c test --csv -f name,gender,age -o test.csv    \n",
    "\n",
    "- MySQL\n",
    "最流行的开源的关系型数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7ff8b87fad48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost', 27017)  #连接数据库\n",
    "mydb = client['mydb']  #新建mydb数据库\n",
    "test = mydb[\"test\"]  #新建test数据集合\n",
    "# 删除一个集合中的所有文档\n",
    "result = test.delete_many({})\n",
    "test.insert_one({\"name\":\"Ming\", \"gender\":\"male\", \"age\":\"30\"})\n",
    "## 在MongoDB设置的路径下， 打开命令行窗口，运行以下命令导出数据\n",
    "# -d： 数据库， -c： 数据集合， -f： 导出的字段\n",
    "# mongoexport -d mydb -c test --csv -f name,gender,age -o test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task： 爬取豆瓣音乐Top250、豆瓣电影Top250\n",
    "方法是： Requests + Lxml + re + **MongoDB**\n",
    "- 代码： [doubanmusic.py](./book_src/doubanmusic.py)\n",
    "- 修复了一个bug， 网页代码有变化\n",
    "- 导出结果： mongoexport -d mydb -c musictop --csv -f name,author,style,time,publisher,score -o douban_music.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多进程爬虫\n",
    "可以使用Python的multiprocessing库的进程池方法进行多进程爬虫。\n",
    "\n",
    "### 实践Task： 爬取58同城二手市场的商品信息\n",
    "方法是： Requests + Lxml + **MongoDB + Muitiprocessing/Pool**  \n",
    "- 代码： [58project/main.py](./book_src/58project/main.py)\n",
    "- 可以参考的一个思想，也是在实践中常常遇到的问题： 如果由于某些原因，爬虫程序中断，如何**不手动更改却可以实现断点续爬**？  \n",
    "代码中的实现方法是一个参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#导入multiprocessing库的Pool模块\n",
    "\n",
    "#创建进程池，processes参数为设置进程的个数\n",
    "pool = Pool(processes=4)\n",
    "\n",
    "# 用map()函数进行进程运行\n",
    "# func参数为需运行的函数，在爬虫实战中，为爬虫函数\n",
    "# iterable为迭代参数，在爬虫实战中，可为多个URL列表进行迭代。\n",
    "pool.map(func,urls)\n",
    "'''\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步加载\n",
    "异步加载技术（AJAX），即异步JavaScript和XML，是指一种创建交互式网页应用的网页开发技术。通过在后台与服务器进行少量数据交换，AJAX可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。  \n",
    "- 通过下滑进行浏览，并没有分页的信息，而是一直浏览下去，而网址信息并没有改变，可以判断该网页使用了异步加载技术\n",
    "- 也可以通过查看数据是否在**网页源代码**中来判断网页是否采用了异步加载技术。下滑后的信息并不在网页源代码中，以此判断使用了异步加载技术  \n",
    "\n",
    "对于异步加载网页，之前的方法是无法获取到页面信息的，那么如何爬取异步加载的网页数据呢？\n",
    "- 想要抓取这些通过异步加载方法的网页数据，需要了解网页是如何加载这些数据的，该过程就叫做逆向工程\n",
    "- **浏览器的Network选项卡**可以查看网页加载过程中的所有文件信息，通过对这些文件的查看和筛选，找出需抓取数据的加载文件，以此来设计爬虫代码\n",
    "- 分页文件大部分在Network选项卡中的XHR选项。通过查看加载时请求的URL，找到规律且简化的URL，然后用之前的爬虫方法\n",
    "\n",
    "### 实践Task：爬取简书用户动态信息\n",
    "- url： http://www.jianshu.com/u/9104ebf5e177\n",
    "- 方法是： Requests + Lxml + MongoDB\n",
    "- 代码： [jianshu_timeline.py](./book_src/jianshu_timeline.py)\n",
    "- 网页域名构造已经发生变化，没有了复杂的动态id，直接可以得到page信息  \n",
    "```https://www.jianshu.com/u/9104ebf5e177?order_by=shared_at&page=8```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表单交互与模拟登录\n",
    "#### Requests库的POST方法\n",
    "#### Cookie模拟登录\n",
    "Cookie指网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据。  \n",
    "可以提交Cookie模拟登录新浪微博、豆瓣等网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 书中的豆瓣登录方法，但现在这种方法已经行不通，改为Cookie模拟登录了\n",
    "#+ Requests的POST方法\n",
    "import requests\n",
    "url = 'https://www.douban.com/accounts/login'\n",
    "params = {\n",
    "    'username':'**',\n",
    "    'password':'**'\n",
    "}\n",
    "html = requests.post(url, data=params)\n",
    "\n",
    "## Cookie登录豆瓣，Requests的GET方法\n",
    "import requests\n",
    "url = 'https://www.douban.com/'\n",
    "headers = {\n",
    "    \"Cookie\":'**'\n",
    "}\n",
    "html = requests.get(url, headers=headers)\n",
    "#print(html.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践Task：爬取拉勾网招聘信息\n",
    "- url： http://www.lagoucom/\n",
    "- 方法是： Requests + 异步加载 + 表单交互 + MongoDB\n",
    "- **※推荐这个任务，看似不同寻常，但并没有相信中那么复杂**。掌握这个任务，可以应对很多相似的问题。\n",
    "    - 异步加载 + 提交表单， 获得翻页后的Response\n",
    "    - 异步加载的URL\n",
    "    - 提交表单，用POST直接获取Response，json格式数据，直接解析就可以得到有效信息  \n",
    "    **当JSON格式很复杂时，可通过“Preview”标签来观察**。招聘信息在content-positionResult-result中。\n",
    "- 代码示例如下所示，完整版代码： [lagou.py](./book_src/lagou.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "## MongoDB\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client['mydb']\n",
    "lagou = mydb['lagou']\n",
    "\n",
    "headers = {\n",
    "    'Cookie':'**',\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'Connection':'keep-alive'\n",
    "}\n",
    "\n",
    "def get_page(url,params):\n",
    "    ## 异步加载 + 提交表单， 获得翻页后的Response\n",
    "    html = requests.post(url, data=params, headers=headers)\n",
    "    ## json\n",
    "    json_data = json.loads(html.text)\n",
    "    total_Count = json_data['content']['positionResult']['totalCount']\n",
    "    ## 获取总页数\n",
    "    page_number = int(total_Count/15) if int(total_Count/15)<30 else 30\n",
    "    get_info(url,page_number)\n",
    "\n",
    "def get_info(url,page):\n",
    "    for pn in range(1,page+1):\n",
    "        # 表单\n",
    "        params = {\n",
    "            'first': 'true',\n",
    "            'pn': str(pn),\n",
    "            'kd': 'Python'\n",
    "        }\n",
    "        try:\n",
    "            ## 异步加载 + 提交表单， 获得翻页后的Response\n",
    "            ## 用POST直接获取Response，json格式数据，直接解析就可以得到有效信息\n",
    "            html = requests.post(url,data=params,headers=headers)\n",
    "            json_data = json.loads(html.text)\n",
    "            results = json_data['content']['positionResult']['result']\n",
    "            ## json解析\n",
    "            for result in results:  ## 省略了很多infos的元素\n",
    "                infos = {\n",
    "                    'businessZones':result['businessZones'],\n",
    "                    'city':result['city'],\n",
    "                }\n",
    "                lagou.insert_one(infos)\n",
    "                time.sleep(2)\n",
    "        ## try/except\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 异步加载的URL\n",
    "    url = 'https://www.lagou.com/jobs/positionAjax.json'\n",
    "    params = {\n",
    "        'first': 'true',\n",
    "        'pn': '1',\n",
    "        'kd': 'Python'\n",
    "    }\n",
    "    # 为了不报错注释， 实际运行不注释\n",
    "    # get_page(url,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium模拟浏览器\n",
    "Selenium是一个用于Web应用程序测试的工具。Selenium直接运行在浏览器中，就像真正的用户在操作一样。  \n",
    "由于这个性质，Selenium也是一个强大的网络数据采集工具，其可以让浏览器自动加载页面，使用了异步加载技术的网页也可获取数据。\n",
    "- pip3 install selenium\n",
    "- Selenium自己不带浏览器，需要配合第三方浏览器来使用。常用的浏览器： Firefox、Chrome、PhantomJS\n",
    "- PhantomJS是无界面的，开销小，速度快\n",
    "- Selenium和PhantomJS的配合使用可以完全模拟用户在浏览器上的所有操作，包括输入框内容填写、单击、截屏、下滑等各种操作。对于需要登录的网站，用户可以不需要通过构造表单或提交cookie信息来登录网站。  \n",
    "\n",
    "一般不到万不得己（异步加载+表达交互依然无法解决问题），基本上不会使用Selenium的方式。  \n",
    "Learn by doing， 遇到具体问题再来看语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Selenimu模拟浏览器， 登录豆瓣\n",
    "from selenium import webdriver\n",
    "## 当使用Firfox或者Chrome浏览器时，需要下载geckodriver\n",
    "#+ 并添加到PATH或者 mv geckodrive /usr/local/bin/\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('https://www.douban.com/accounts/login')\n",
    "driver.implicitly_wait(10)\n",
    "driver.find_element_by_class_name('account-tab-account').click()\n",
    "driver.find_element_by_id('username').clear()\n",
    "driver.find_element_by_id('username').send_keys('123456')\n",
    "driver.find_element_by_id('password').clear()\n",
    "driver.find_element_by_id('password').send_keys('abc123')\n",
    "driver.implicitly_wait(2)\n",
    "driver.find_element_by_class_name('account-form-field-submit ').click()\n",
    "#print(driver.page_source)\n",
    "driver.implicitly_wait(10)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy爬虫框架\n",
    "非常好用的爬虫框架，但此书的讲解和例子不够好，不再列出笔记。  \n",
    "后续通过其他资料学习和给出实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 小结\n",
    "### 爬虫原理\n",
    "结合实例，了解搭建爬虫的逻辑和思路，分析和拆解问题。\n",
    "\n",
    "### 爬虫三大库\n",
    "- Requests， GET和POST请求\n",
    "- Lxml， Xpath方法\n",
    "- BeautifulSoup， select方法  \n",
    "\n",
    "### Python模块\n",
    "- re\n",
    "- json\n",
    "- time、 csv、 try/except、 pymongo、 multiprocessing\n",
    "- Selenium\n",
    "\n",
    "### 除了三大库，还有与爬虫相关的方方面面、细节 & 技巧\n",
    "- 数据库MongoDB\n",
    "- 多进程/多线程\n",
    "- 异常判断\n",
    "- 断点自动续爬\n",
    "- **异步加载**\n",
    "- **表单交互**\n",
    "- **Cookie模拟登录**\n",
    "- Selenium模拟浏览器\n",
    "- Scrapy爬虫框架\n",
    "\n",
    "### 爬虫实例汇总\n",
    "- 爬取糗事百科的段子\n",
    "- 爬取《斗破苍穹》全文小说\n",
    "- 爬取酷狗Top500的数据\n",
    "- 爬取豆瓣图书Top250、豆瓣音乐Top250、豆瓣电影Top250\n",
    "- 使用API爬取数据：百度地图、高德地图\n",
    "- 爬取58同城二手市场的商品信息\n",
    "- 爬取简书用户动态信息\n",
    "- 爬取拉勾网招聘信息\n",
    "\n",
    "### 爬虫常见方法\n",
    "- Requests + Lxml/re + MongoDB + Muitiprocessing/Pool\n",
    "- Requests + 异步加载 + 表单交互 + MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 与爬虫or文本处理有关的Python模块/库\n",
    "import requests\n",
    "import re\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import pymongo\n",
    "from multiprocessing import Pool\n",
    "from selenium import webdriver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
